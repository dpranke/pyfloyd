# Copyright 2025 Dirk Pranke. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# This file contains the code generation templates for Python3.
# It depends on a base set of templates used by fairly similar languages.

name = 'Python'
ext = '.py'
indent = 4
line_length = 79

starting_template: 'generate'

inherit: ['base.dft']

# Python doesn't need to declare local vars
declare_local_vars: false

templates: {

#
# Helper rules
#

block: [fn [head body] [vl [hl head ':'] [ind body]]]


# Reference different kinds of variable names.

fld: [fn [name] [strcat 'self._' name]]

method_name: [fn [name] [fld name]]

extern: [fn [name] [strcat 'self._externs[' [lit name] ']']]

# Returns a field or method on the generated parse object.
f_state: [fld 'state']
f_errpos: [fld 'errpos']
f_pos: [strcat f_state '.pos']
f_val: [strcat f_state '.val']
failed: [strcat f_state '.failed']

save: [fn [var] [hl var ' = self._state.copy()' t_end]]

#
# Helpers for generating text portably
#

t_assign: [fn [var val] [hl var ' = ' val t_end]]
t_assign_outer: [fn [var val]
                    [hl "self._scopes[-1]['" var "'] = " val]]
t_append: [fn [lst val] [hl lst '.append(' val ')' t_end]]
t_break:   'break'
t_end: ''  # statement terminator (not used in Python)
t_false: 'False'
t_list_one_any: [fn [x] [hl '[' x ']']]
t_list_zero_any: '[]'
t_meth_def: 'def _'
t_meth_self: 'self'
t_meth_params: [fn [params]
                   [if [equal params ''] 'self' [strcat 'self, ' params]]]
t_eq: [fn [x y] [hl x ' == ' y]]
t_if: [fn [cond body] [block [hl 'if ' cond] body]]
t_ifelse: [fn [cond if_body e_body]
              [vl [block [hl 'if ' cond] if_body]
                  [block 'else' e_body]]]
t_ifelifelse: [fn [cond if_body elif_cond elif_body e_body]
              [vl [block [hl 'if ' cond] if_body]
                  [block [hl 'elif ' elif_cond] elif_body]
                  [block 'else' e_body]]]
t_istrue: [fn [v] [hl v ' is True']]
t_isfalse: [fn [v] [hl v ' is False']]
t_declare_local_vars: [fn [node] [vl]]
t_declare_node_vars: [fn [node] [vl]]
t_or: [fn [x y] [hl x ' or ' y]]
t_newscope: '{}'  # value for a new scope (dict[str, any])
t_not: 'not '  # the logical operator
t_null: 'None'
t_pop: [fn [lst] [hl lst '.pop()']]
t_return:  'return'
t_self: 'self'
t_substr: [fn [s start end] [hl s '[' start ':' end ']']]
t_throw: [fn [msg] [hl 'raise _ParsingRuntimeError(' msg ')' t_end]]
t_to_str: [fn [x] x]
t_toplevel_extra_sep: ''
t_true: 'True'
t_while: [fn [cond body] [block [hl 'while ' cond] body]]

#
# The basic block structure of the file is given in the `generate` template
# in base. This section defines the templates `generate` references.
#

default_header: @"""
    # Generated by pyfloyd version @generator_options.version
    #    https://github.com/dpranke/pyfloyd
    #
    @wrap_argv['#    `' '#     ']
    #

    @imports

    Externs = Optional[Dict[str, Any]]

    # pylint: disable=too-many-lines
    """

main_header: @"""
    #!/usr/bin/env python3
    @default_header[]
    """

imports: [vl [if generator_options.main
                 [vl 'import argparse'
                     'import json'
                     'import os'
                     'import sys']]
             [if grammar.re_needed 'import re']
             'from typing import Any, Dict, NamedTuple, Optional'
             [if grammar.unicodedata_needed 'import unicodedata']]


parsing_runtime_exception_class: """
    class _ParsingRuntimeError(Exception):
        pass
    """

state_class: @"""
    class _State:
        pos: int = 0
        val: any = None
        failed: bool = False
        @if[grammar.tokenize [vl 'cur_token = 0' 'in_token = False']]

        def __repr__(self):
            return (f'_State(pos={self.pos}, failed={self.failed})')

        def copy(self):
            r = _State()
            r.pos = self.pos
            r.val = self.val
            r.failed = self.failed
            @if[grammar.tokenize [vl 'r.cur_token = self.cur_token'
                                     'r.in_token = self.in_token']]
            return r
    """

operator_state_class: """
    class _OperatorState:
        current_depth: int = 0
        current_prec: int = 0
        prec_ops: dict[int, str] = {}
        precs: list[int] = []
        rassoc: set[str] = set()
        choices: dict[str, Any] = {}  # really dict[str, grammar.Rule]
    """  # operator_state_class

result_class: '''
    class Result(NamedTuple):
        """The result returned from a `parse()` call.

        If the parse is successful, `val` will contain the returned value, if
        any and `pos` will indicate the point in the text where the parser
        stopped.  If the parse is unsuccessful, `err` will contain a string
        describing any errors that occurred during the parse and `pos` will
        indicate the location of the farthest error in the text.
        """

        val: Any = None
        err: Optional[str] = None
        pos: Optional[int] = None
    '''  # result_class


parse_function: '''
    def parse(
        text: str, path: str = '<string>', externs: Externs = None, start: int = 0
    ) -> Result:
        """Parse a given text and return the result.

        If the parse was successful, `result.val` will be the returned value
        from the parse, and `result.pos` will indicate where the parser
        stopped when it was done parsing.

        If the parse is unsuccessful, `result.err` will be a string describing
        any errors found in the text, and `result.pos` will indicate the
        furthest point reached during the parse.

        If the optional `path` is provided it will be used in any error
        messages to indicate the path to the filename containing the given
        text.
        """
        return _Parser(text, path).parse(externs, start)
    '''  # parse_function

parser_class: @"""
    class _Parser:
        @parser_methods[]
    """

parser_constructor: @"""
    def __init__(self, text, path):
        self._text = text
        self._end = len(self._text)
        self._errpos = 0
        self._path = path
        self._state = _State()
        @externs[]
        @if[generator_options.memoize 'self._cache = {}']
        @if[grammar.seeds_needed 'self._seeds = {}']
        @if[grammar.leftrec_needed 'self._blocked = set()']
        @if[grammar.re_needed 'self._regexps = {}']
        @if[grammar.lookup_needed 'self._scopes = []']
        @if[grammar.operator_needed
            [vl 'self._operators = {}'
                [map_items operator_state grammar.operators]]]
        @if[grammar.tokenize
            [vl 'self._nodes = []'
                'self._tokens = []'
                'self._token_rules = ['
                [ind [comma [map [fn [t] [lit t]] [sort grammar.tokens]]]]
                ']']]

    def pos(self):
        return self._state.pos

    """  # parser_constructor

externs: [fn [] [if [dict_is_empty grammar.externs]
                    [vl 'self._externs = {}']
                    [vl 'self._externs = {'
                        [ind [map_items [fn [k v]
                                            [hl [lit k]
                                                ": "
                                                [if [or [equal v 'func']
                                                        [equal v 'pfunc']]
                                                    [if [has functions k]
                                                        [fld [fn_name k]]
                                                        'None']
                                                    [if v t_true t_false]]
                                                ","]]
                                        grammar.externs]]
                        '}']]]

operator_state: [fn [rule o]
                    [vl 'o = _OperatorState()'
                        [operator_prec_ops o]
                        'o.precs = sorted(o.prec_ops, reverse=True)'
                        [operator_rassoc o]
                        [operator_choices o]
                        [hl 'self._operators[' [lit rule] '] = o']]]

operator_prec_ops: [fn [o]
                       [vl 'o.prec_ops = {'
                           [ind [map [fn [prec]
                                       [hl [itoa prec]
                                           ': ['
                                           [map [fn [op] [lit op]]
                                                [get o.prec_ops prec]
                                                ', ']
                                           '],']]
                                     [sort [keys o.prec_ops]]]]
                           '}']]

operator_rassoc: [fn [o]
                     [hl 'o.rassoc = set(['
                         [comma  [map [fn [op] [strcat "'" op "'"]]
                                      o.rassoc]]
                         '])']]

operator_choices: [fn [o] [vl 'o.choices = {'
                              [ind [map_items [fn [op meth]
                                                  [hl [lit op]
                                                      ': self._'
                                                      meth
                                                      ',']]
                                              o.choices]]
                              '}']]

parse_with_exception: @"""
    def parse(self, externs: Externs = None, start: int = 0):
        self._state.pos = start
        @if[grammar.tokenize 'self._state.cur_token = 0']

        errors = ''
        if externs:
            for k, v in externs.items():
                if k in self._externs:
                    self._externs[k] = v
                else:
                    errors += f'Unexpected extern "{k}"\n'
        for k, v in self._externs.items():
            if v is None:
                errors += f'Missing required extern "{k}"'
        if errors:
            return Result(None, errors, 0)

        try:
            self._r_@grammar.starting_rule()

            if self._state.failed:
                return Result(None, self._o_error(), self._errpos)
            return Result(self._state.val, None, self._state.pos)
        except _ParsingRuntimeError as e:  # pragma: no cover
            lineno, _ = self._o_offsets(self._errpos)
            return Result(
                None,
                self._path + ':' + str(lineno) + ' ' + str(e),
                self._errpos,
            )
    """

parse_without_exception: @"""
    def parse(self, externs: Externs = None, start: int = 0):
        self._state.pos = start
        @if[grammar.tokenize 'self._state.cur_token = 0']

        errors = ''
        if externs:
            for k, v in externs.items():
                self._externs[k] = v
        for k, v in self._externs.items():
            if v is None:
                errors += f'Missing required extern "{k}"'
        if errors:
            return Result(None, errors, 0)

        self._r_@grammar.starting_rule()

        if self._state.failed:
            return Result(None, self._o_error(), self._errpos)
        return Result(@f_val, None, @f_pos)
    """

builtin_fn: [fn [name]
                [if [and [has functions name]
                         [has [get functions name] 'body']]
                    [vl ''
                        [def_method [fn_name name]
                                    [function_param_names name]
                                    ''
                                    [get [get functions name] 'body']]]
                    [vl '' [invoke [fn_name name]]]]]

function_param_names: [fn [name] [map [fn [p] [item p 0]]
                                 [get [get functions name] 'params']]]

default_footer: null

main_footer: """


    def main(
        argv=sys.argv[1:],
        stdin=sys.stdin,
        stdout=sys.stdout,
        stderr=sys.stderr,
        exists=os.path.exists,
        opener=open,
    ) -> int:
        arg_parser = argparse.ArgumentParser()
        arg_parser.add_argument('-c', '--code')
        arg_parser.add_argument(
            '-D',
            '--define',
            action='append',
            metavar='var=val',
            default=[],
            help='define an external var=value (may use multiple times)'
        )
        arg_parser.add_argument('file', nargs='?')
        args = arg_parser.parse_args(argv)

        if args.code is not None:
            msg = args.code
            path = '<code>'
        elif not args.file or args.file[1] == '-':
            path = '<stdin>'
            fp = stdin
        elif not exists(args.file):
            print('Error: file "%s" not found.' % args.file, file=stderr)
            return 1
        else:
            path = args.file
            fp = opener(path)

        externs = {}
        for d in args.define:
            k, v = d.split('=', 1)
            externs[k] = json.loads(v)

        if args.code is None:
            msg = fp.read()
        result = parse(msg, path, externs)
        if result.err:
            print(result.err, file=stderr)
            return 1
        print(json.dumps(result.val, indent=2), file=stdout)
        return 0


    if __name__ == '__main__':
        sys.exit(main())
    """  # main_footer

#
# Language-specific rules for nodes in the AST.
#

n_regexp: [fn [node]
              [vl [t_assign 'rexp' [lit node.v]]
                  'pos = self.pos()'
                  'if rexp not in self._regexps:'
                  '    self._regexps[rexp] = re.compile(rexp)'
                  'm = self._regexps[rexp].match(self._text, pos)'
                  'if m:'
                  [ind 'self._o_succeed(m.group(0), m.end())'
                       [if grammar.tokenize
                           "self._o_tok(pos, 'regexp')"]
                       'return']
                  [fail]]]

n_rule_wrapper: [fn [node]
                    [let [[is_tok [in grammar.tokens node.v]]]
                      [vl [t_declare_local_vars node]
                          [strcat "self._nodes.append((self.pos(), '"
                                  node.v
                                  "'))"]
                          [if is_tok 'self._state.in_token = True']
                          [stmts node.child]
                          [if is_tok
                              [vl 'self._state.in_token = False'
                                  [if [or [equal node.v '%whitespace']
                                          [equal node.v '%comment']]
                                      [hl 'self._o_tok('
                                          'self._nodes[-1][0], ',
                                          'self._nodes[-1][1])'
                                               ]]]]
                          "self._nodes.pop()"]]]

n_set: [fn [node]
           [vl [t_assign 'rexp' [lit [strcat '[' node.v ']']]]
               'pos = self.pos()'
               [t_if 'rexp not in self._regexps'
                     'self._regexps[rexp] = re.compile(rexp)']
               'm = self._regexps[rexp].match(self._text, pos)'
               [t_if 'm'
                     [vl [succeed 'm.group(0)' 'm.end() + 1']
                         [if grammar.tokenize
                             "self._o_tok(pos, 'set')"]
                         t_return]]
               [fail]]]

#
# Built-in operators and rules
#

r_any: [
    meth q[]
        [vl 'pos = self.pos()'
            'if pos < self._end:'
            [ind 'self._o_succeed(self._text[pos], pos + 1)'
                 [if grammar.tokenize "self._o_tok(pos - 1, 'any')"]]
            'else:'
            [ind 'self._o_fail()']]]

r_end: [meth q[] "
             if self.pos() == self._end:
                 self._o_succeed(None, self.pos())
             else:
                 self._o_fail()
             "]

o_ch: [meth q['ch']
            [vl 'pos = self.pos()'
                'if pos < self._end and self._text[pos] == ch:'
                [ind 'self._o_succeed(ch, pos + 1)'
                     [if grammar.tokenize "self._o_tok(pos, 'lit')"]]
                'else:'
                [ind 'self._o_fail()']]]

o_offsets: [meth q['pos'] '''
                lineno = 1
                colno = 1
                for i in range(pos):
                    if self._text[i] == '\\n':
                        lineno += 1
                        colno = 1
                    else:
                        colno += 1
                return lineno, colno
                ''']

o_error: [meth q[] '''
              lineno, colno = self._o_offsets(self._errpos)
              if self._errpos == len(self._text):
                  thing = 'end of input'
              else:
                  thing = repr(self._text[self._errpos]).replace("'", '"')
              path = self._path
              return f'{path}:{lineno} Unexpected {thing} at column {colno}'
              ''']

o_fail: [meth q[] '''
              self._state.val = None
              self._state.failed = True
              self._errpos = max(self._errpos, self._state.pos)
              ''']

o_leftrec: [meth q['rule' 'rule_name' 'left_assoc'] '''
                  pos = self.pos()
                  key = (rule_name, pos)
                  seed = self._seeds.get(key)
                  if seed:
                      self._state = seed
                      return
                  if rule_name in self._blocked:
                      self._state.val = None
                      self._state.failed = True
                      return
                  current = self._state
                  self._seeds[key] = current
                  if left_assoc:
                      self._blocked.add(rule_name)
                  while True:
                      rule()
                      if self._pos > current[2]:
                          current = self._state
                          self._seeds[key] = current
                          self._pos = pos
                      else:
                          del self._seeds[key]
                          self._state = current
                          if left_assoc:
                              self._blocked.remove(rule_name)
                          return
                  ''']

o_lookup: [meth q['var'] '''
                i = len(self._scopes) - 1
                while i >= 0:
                    if var in self._scopes[i]:
                        return self._scopes[i][var]
                    i -= 1
                if var in self._externs:
                    return self._externs[var]
                assert False, f'unknown var {var}'
                ''']

o_memoize: [meth q['rule_name' 'fn'] '''
        pos = self.pos()
        if pos not in self._cache:
            self._cache[pos] = {}
        c = self._cache[pos]
        if rule_name in c:
            self._state = c[rule_name]
            return
        fn()
        c[rule_name] = self._state
        ''']

o_operator: [meth q['rule_name'] '''
        o = self._operators[rule_name]
        state = self._state
        pos = state.pos
        cache = self._seeds.get(pos)
        if not cache:
            self._seeds[pos] = {}
        cache = self._seeds[pos]
        entry = cache.get(rule_name)
        if entry:
            self._state = entry
            return
        o.current_depth += 1
        entry = self._state
        cache[rule_name] = entry
        min_prec = o.current_prec
        i = 0
        while i < len(o.precs):
            repeat = False
            prec = o.precs[i]
            prec_ops = o.prec_ops[prec]
            if prec < min_prec:
                break
            o.current_prec = prec
            if prec_ops[0] not in o.rassoc:
                o.current_prec += 1
            for j, _ in enumerate(prec_ops):
                op = prec_ops[j]
                o.choices[op]()
                if not self._state.failed and self.pos() > pos:
                    entry = self._state
                    cache[rule_name] = entry
                    repeat = True
                    break
                self._o_restore(state)
            if not repeat:
                i += 1

        del cache[rule_name]
        o.current_depth -= 1
        if o.current_depth == 0:
            o.current_prec = 0
        self._state = entry
        ''']

o_range: [meth q['i' 'j'] '''
               pos = self.pos()
               if pos != self._end and ord(i) <= ord(self._text[pos]) <= ord(j):
                   self._o_succeed(self._text[pos], pos + 1)
               else:
                   self._o_fail()
               ''']

o_restore: [meth q['state']
                 [vl 'self._state = state'
                     [if grammar.tokenize
                         [vl [strcat 'while self._tokens and '
                                     'self._tokens[-1][0] > self._state.pos:']
                             [ind 'self._tokens.pop()']]]]]

o_str: [meth q['s']
             [vl [if grammar.tokenize
                     """
                     in_token = self._state.in_token
                     self._state.in_token = True
                     pos = self.pos()
                     """]
                 """
                 for ch in s:
                     self._o_ch(ch)
                     if self._state.failed:
                         return
                 self._state.val = s
                 """
                 [if grammar.tokenize
                     """
                     self._state.in_token = in_token
                     self._o_tok(pos, 'lit')
                     """]]]

o_succeed: [meth q['v' 'newpos'] "
                self._state.val = v
                self._state.failed = False
                self._state.pos = newpos
                "]

o_tok: [meth q['pos' 'tag']
             """
             if self._state.in_token:
                 return
             if not self._state.failed and self.pos() > pos:
                 val = (pos, tag, self._text[pos:self.pos()])
                 if self._tokens and self._tokens[-1][0] == pos:
                     assert self._tokens[-1] == val
                 else:
                     self._tokens.append((pos, tag, self._text[pos:self.pos()]))
             """]

o_unicat: [
    meth q['cat'] "
         pos = self.pos()
         if pos < self._end and unicodedata.category(self._text[pos]) == cat:
             self._o_succeed(self._text[pos], pos + 1)
         else:
             self._o_fail()
         "]

fn_colno: [
    meth q[] """
            colno = 0
            pos = self.pos()
            if pos == self._end:
                colno += 1
            while pos >= colno and self._text[pos - colno] != '\\n':
                colno += 1
            return colno
            """]

fn_failed: [meth q[] 'return self._state.failed']

fn_pos: [meth q[] 'return self.pos()']

fn_node: [meth q['parser' '*args'] """
        del parser
        return args[0]
        """]

}
